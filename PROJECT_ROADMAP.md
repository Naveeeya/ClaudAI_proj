# ğŸ¯ Pitch Slap - Complete Project Roadmap

**Hackathon**: RunAnywhere AI x Firebender Challenge  
**Track**: PS 3 - The Zero-Latency Voice Interface  
**Team Size**: 3 Members  
**Objective**: Build a real-time language practice partner with instant pronunciation feedback

---

## ğŸš¦ Project Status at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROGRESS: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40% Complete           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½ï¿½ï¿½â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  âœ… Phase 1: Architecture Refactoring     [DONE] 100%       â”‚
â”‚  âœ… Phase 2: Voice Activity Detection     [DONE] 100%       â”‚
â”‚  âœ… Phase 3: Interrupt Logic              [DONE] 100%       â”‚
â”‚  â¬œ Phase 0: Project Cleanup              [TODO] START HERE â”‚
â”‚  â¬œ Phase 4: Audio Recording              [TODO]            â”‚
â”‚  â¬œ Phase 5: AI Integration (Whisper+LLM) [TODO]            â”‚
â”‚  â¬œ Phase 6: Conversation Manager         [TODO]            â”‚
â”‚  â¬œ Phase 7: Production UI                [TODO]            â”‚
â”‚  â¬œ Phase 8: Documentation & Submission   [TODO]            â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ESTIMATED REMAINING TIME: 33-45 hours                      â”‚
â”‚  CRITICAL PATH: Phase 0 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8                â”‚
â”‚  BLOCKER: None (Ready to proceed)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Next Immediate Action

**âš ï¸ START WITH PHASE 0 (Project Cleanup) - 30 minutes**

Before writing any code, we must:

1. Archive old documentation files
2. Delete placeholder code
3. Create file inventory for AI agent
4. Validate project structure

**ğŸ‘‰ Jump to:** [Phase 0: Project Cleanup](#phase-0-project-cleanup--alignment--start-here---critical)

---

## ğŸ† What Makes This Project Win-Worthy

| Criteria                           | Our Advantage                           | Evidence                                   |
|------------------------------------|-----------------------------------------|--------------------------------------------|
| **Technical Implementation (30%)** | Advanced voice pipeline with barge-in   | VAD + InterruptLogic both production-ready |
| **On-Device Necessity (25%)**      | Sub-80ms latency impossible with cloud  | VAD achieves ~20ms detection               |
| **Innovation (25%)**               | Real-time pronunciation coach is unique | No existing app does this offline          |
| **Documentation (10%)**            | Professional README + demo video        | Comprehensive docs already                 |
| **Engagement (10%)**               | Active development visible              | Clean commit history                       |

**ğŸ¯ Judge Appeal**: "Privacy-first, zero-cost language learning with instant feedback"

---

## ğŸ“‹ Table of Contents

1. [ğŸ¤– AI Agent Execution Guide](#ai-agent-execution-guide) â­ **READ THIS FIRST**
2. [Project Overview](#project-overview)
3. [Current Progress](#current-progress)
4. [Technical Architecture](#technical-architecture)
5. [Phase-by-Phase Roadmap](#phase-by-phase-roadmap)
6. [Team Task Distribution](#team-task-distribution)
7. [Final Deliverables Checklist](#final-deliverables-checklist)
8. [Testing & Quality Assurance](#testing--quality-assurance)
9. [Timeline & Milestones](#timeline--milestones)

---

## ğŸ¤– AI Agent Execution Guide

**âš ï¸ CRITICAL: Read this section before executing any phase**

This roadmap is designed for autonomous execution by an AI coding agent (Claude/Firebender). Follow these rules strictly
to ensure success.

---

### Core Execution Principles

#### 1. **Sequential Execution Only**

```
Phase 0 â†’ Phase 4 â†’ Phase 5 â†’ Phase 6 â†’ Phase 7 â†’ Phase 8
   â†“         â†“         â†“         â†“         â†“         â†“
 CLEAN    AUDIO     AI INT   CONVO    UI      DOCS
```

**NEVER skip phases or work out of order**. Each phase depends on the previous one.

---

#### 2. **One File at a Time**

For each task:

1. Read the task requirements completely
2. Create/modify ONE file
3. Verify it compiles (`./gradlew build`)
4. Test if applicable
5. Move to next file

**DO NOT** create multiple files simultaneously. This prevents cascading errors.

---

#### 3. **Always Check File Inventory**

Before creating any file, consult `PROJECT_FILE_INVENTORY.md` (created in Phase 0):

- Verify the file doesn't already exist
- Confirm you're using the correct package path
- Check dependencies are complete

---

#### 4. **Comprehensive Testing**

After each component:

- Build the project (`./gradlew build`)
- Run on device/emulator if applicable
- Check Logcat for errors (`adb logcat -s PitchSlap_*`)
- Verify integration with existing components

---

#### 5. **Detailed Logging**

Every class should include:

```kotlin
companion object {
    private const val TAG = "PitchSlap_[ComponentName]"
}
```

Log key events:

- Initialization: `Log.i(TAG, "âœ… Component initialized")`
- State changes: `Log.d(TAG, "State changed to: $newState")`
- Errors: `Log.e(TAG, "âŒ Error: ${e.message}", e)`

This makes debugging trivial via Logcat.

---

#### 6. **Error Handling Everywhere**

Every external operation must have try-catch:

```kotlin
suspend fun riskyOperation() {
    try {
        // Operation that might fail
        val result = externalAPI.call()
        Log.i(TAG, "âœ… Operation succeeded")
    } catch (e: Exception) {
        Log.e(TAG, "âŒ Operation failed: ${e.message}", e)
        // Graceful fallback or user notification
    }
}
```

**Never let exceptions crash the app**.

---

#### 7. **Code Quality Standards**

**Required for all code**:

- âœ… KDoc comments on all public functions/classes
- âœ… Meaningful variable names (no `x`, `temp`, `data`)
- âœ… Proper null safety (`?`, `!!` only when justified)
- âœ… Coroutine best practices (use appropriate Dispatcher)
- âœ… StateFlow for UI-reactive data
- âœ… Immutable data classes where possible

**Example**:

```kotlin
/**
 * Captures user audio during speech for transcription
 * 
 * Uses AudioRecord API to stream PCM audio at 16kHz, 16-bit, mono.
 * Integrates with VoiceActivityDetection to start/stop recording.
 * 
 * @see VoiceActivityDetection for speech detection
 */
class AudioRecorder {
    companion object {
        private const val TAG = "PitchSlap_AudioRecorder"
        private const val SAMPLE_RATE = 16000
        private const val CHANNEL_CONFIG = AudioFormat.CHANNEL_IN_MONO
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT
    }
    
    // ... implementation
}
```

---

#### 8. **Self-Verification Checklist**

After completing each task, verify:

**Compilation**:

- [ ] `./gradlew build` succeeds
- [ ] No compilation errors
- [ ] No unresolved references

**Code Quality**:

- [ ] All imports are specific (no wildcards)
- [ ] KDoc comments on public APIs
- [ ] Proper error handling (try-catch)
- [ ] Logging at key points

**Integration**:

- [ ] New component connects to existing ones
- [ ] Dependencies are injected properly
- [ ] StateFlow/Flow APIs are correct

**Testing**:

- [ ] Component can be instantiated
- [ ] Methods execute without crashing
- [ ] Logcat shows expected output

---

#### 9. **Reference Materials**

For each phase, consult these documents:

**Phase 4 (Audio Recording)**:

- `VoiceActivityDetection.kt` (existing file, use as reference)
- Android AudioRecord documentation

**Phase 5 (AI Integration)**:

- `RUNANYWHERE_SDK_COMPLETE_GUIDE.md` (comprehensive SDK reference)
- `PitchSlapApplication.kt` (see how SDK is initialized)

**Phase 6 (Conversation Manager)**:

- `InterruptLogic.kt` (barge-in handling pattern)
- All Phase 4 & 5 files

**Phase 7 (Production UI)**:

- `MainActivity.kt` (existing test UI as reference)
- `app/src/main/java/com/pitchslap/app/ui/theme/` (theme configuration)

**Phase 8 (Documentation)**:

- All implementation summaries in `docs/archive/`
- Existing README.md structure

---

#### 10. **When Things Go Wrong**

**Compilation Error**:

1. Read the error message carefully
2. Check imports and package declarations
3. Verify dependencies in `build.gradle.kts`
4. Sync Gradle (`./gradlew clean build`)

**Runtime Error**:

1. Check Logcat for stack trace
2. Verify permissions in AndroidManifest.xml
3. Ensure coroutines are launched in correct scope
4. Check null safety (add null checks)

**Integration Error**:

1. Verify dependency flow (check file inventory)
2. Ensure StateFlow is properly exposed
3. Check lifecycle management (no leaks)
4. Add more logging to trace issue

**Stuck on a Task**:

1. Re-read the task requirements
2. Check similar existing code as reference
3. Break down into smaller sub-tasks
4. Ask for clarification if truly blocked

---

#### 11. **Communication Protocol**

When completing each task, provide:

1. **Status Update**:
   ```
   âœ… Task 4.1 Complete: AudioRecorder.kt created
   - 156 lines of code
   - Successfully compiles
   - Tested with VAD integration
   - Logcat shows proper audio capture
   ```

2. **Next Steps**:
   ```
   ğŸ”¨ Starting Task 4.2: Audio Recording Test UI
   - Will update MainActivity.kt
   - Add record/playback buttons
   - Expected completion: 30 minutes
   ```

3. **Blockers** (if any):
   ```
   âš ï¸ Potential issue found:
   - AudioRecord requires RECORD_AUDIO permission
   - Need to verify permission is in AndroidManifest.xml
   - Will check before proceeding
   ```

---

#### 12. **Phase Completion Criteria**

Before marking a phase complete:

- [ ] All tasks in the phase are finished
- [ ] All files compile successfully
- [ ] Integration tests pass (if applicable)
- [ ] Logcat shows no errors during execution
- [ ] Code is committed with clear message
- [ ] Phase status updated in PROJECT_ROADMAP.md
- [ ] Ready to proceed to next phase

---

### Quick Start for AI Agent

**To begin execution:**

1. **Read this entire section** (AI Agent Execution Guide)
2. **Execute Phase 0** (Project Cleanup) - creates file inventory
3. **For each subsequent phase**:
   - Read the phase section in this roadmap
   - Check dependencies in file inventory
   - Execute tasks sequentially
   - Test after each task
   - Verify completion criteria
   - Move to next phase

4. **Keep PROJECT_ROADMAP.md open** as primary reference
5. **Use RUNANYWHERE_SDK_COMPLETE_GUIDE.md** for SDK APIs
6. **Refer to existing code** (VAD, InterruptLogic) as patterns

---

### Expected Timeline

| Phase                         | Estimated Time  | Complexity |
|-------------------------------|-----------------|------------|
| Phase 0: Cleanup              | 30 min          | Low        |
| Phase 4: Audio Recording      | 4-6 hours       | Medium     |
| Phase 5: AI Integration       | 8-12 hours      | High       |
| Phase 6: Conversation Manager | 6-8 hours       | High       |
| Phase 7: Production UI        | 8-10 hours      | Medium     |
| Phase 8: Documentation        | 6-8 hours       | Low        |
| **Total**                     | **33-45 hours** | -          |

---

### Success Indicators

You're executing correctly if:

- âœ… Each phase builds on the previous one smoothly
- âœ… No compilation errors at any point
- âœ… Logcat shows expected logs for each component
- âœ… Test UI updates reflect new functionality
- âœ… Code follows consistent patterns
- âœ… Documentation stays up-to-date

---

**ğŸš€ Ready to Begin: Start with Phase 0 (Project Cleanup)**

---

## ğŸ¯ Project Overview

### What We're Building

**Pitch Slap** - A zero-latency voice language practice application that:

- Provides **instant pronunciation feedback** (sub-80ms response time)
- Works **completely offline** after model download
- Features **natural conversation flow** with barge-in interruption
- Uses **on-device AI** for privacy and zero inference costs
- Delivers **professional-grade voice interactions**

### Key Value Propositions

1. âœ… **Sub-80ms Latency**: Feels instantaneous and human
2. âœ… **Complete Privacy**: All processing happens on-device
3. âœ… **Zero Inference Costs**: No cloud API bills
4. âœ… **Works Offline**: No internet needed after setup
5. âœ… **Natural Interruptions**: Users can barge-in like real conversations

### Hackathon Evaluation Criteria (How We'll Be Judged)

| Criteria | Weight | Our Strategy |
|----------|--------|--------------|
| **Technical Implementation** | 30% | Solid SDK integration + advanced voice pipeline |
| **On-Device Necessity** | 25% | Sub-80ms latency impossible with cloud |
| **Innovation & Creativity** | 25% | Real-time pronunciation coach is unique |
| **Presentation & Documentation** | 10% | Professional README + clear demo video |
| **Engagement** | 10% | Active development + documentation |

---

## âœ… Current Progress

### Phase 1: Architecture Refactoring âœ… COMPLETE

**Status**: 100% Done  
**What Was Built**:

- Clean MVVM package structure
- SDK initialization in `PitchSlapApplication.kt`
- Model registration system
- Base MainActivity with Compose UI

**Files**:

- `app/src/main/java/com/pitchslap/app/PitchSlapApplication.kt`
- `app/src/main/java/com/pitchslap/app/MainActivity.kt`
- `app/src/main/java/com/pitchslap/app/ui/theme/*`

---

### Phase 2: Voice Activity Detection (VAD) âœ… COMPLETE

**Status**: 100% Done  
**What Was Built**:

**Core Features**:

- âœ… Real-time audio streaming (16kHz, 16-bit PCM)
- âœ… RMS amplitude calculation
- âœ… Threshold-based speech detection (RMS > 2000)
- âœ… 500ms silence timeout
- âœ… `StateFlow<Boolean> isUserSpeaking` exposed
- âœ… Background coroutine processing
- âœ… Comprehensive logging with emojis

**Technical Details**:

- Sample Rate: 16kHz (optimal for voice)
- RMS Threshold: 2000 (tunable)
- Silence Timeout: 500ms
- Detection Latency: <20ms
- CPU Usage: ~2-5%

**Files**:

- `app/src/main/java/com/pitchslap/app/logic/VoiceActivityDetection.kt` (270 lines)
- `VAD_TESTING_GUIDE.md` (comprehensive testing documentation)
- `VAD_IMPLEMENTATION_SUMMARY.md` (technical summary)

**What's Working**:

- Real-time speech detection visible in UI (green circle when speaking)
- Logcat shows RMS values and state changes
- Microphone permission handling
- Lifecycle management (start/stop/cleanup)

---

### Phase 3: Interrupt Logic (Barge-In Detection) âœ… COMPLETE

**Status**: 100% Done  
**What Was Built**:

**Core Features**:

- âœ… Dependency injection of VAD
- âœ… AI speaking state tracking (`StateFlow<Boolean> isAiSpeaking`)
- âœ… Background monitoring coroutine
- âœ… Conflict detection (user + AI both speaking)
- âœ… Immediate AI cutoff on interrupt
- âœ… Event emission (`SharedFlow<BargeInEvent>`)
- âœ… Statistics tracking (barge-in counter)
- âœ… Helper functions: `startAiSpeech()`, `stopAiSpeech()`

**Technical Details**:

- Detection Latency: <20ms
- Event-driven architecture
- Thread-safe with Kotlin Flows
- Zero false positives/negatives

**Files**:

- `app/src/main/java/com/pitchslap/app/logic/InterruptLogic.kt` (155 lines)
- `INTERRUPT_LOGIC_TESTING_GUIDE.md`
- `INTERRUPT_LOGIC_IMPLEMENTATION_SUMMARY.md`

**What's Working**:

- Animated blue "AI IS TALKING" banner
- Instant disappearance when user speaks
- Barge-in counter increments correctly
- One-click automated testing
- Visual + Logcat verification

**Test UI Features**:

- Large visual indicator (color-changing circle)
- Real-time statistics card
- "Simulate AI Talking" button
- "Simulate User Speaking" button (for emulator testing)
- One-click barge-in test
- Comprehensive test instructions

---

## ğŸ—ï¸ Technical Architecture

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PITCH SLAP VOICE SYSTEM                      â”‚
â”‚                    (On-Device AI Application)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  Microphone  â”‚ ğŸ¤                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  VoiceActivityDetection.kt             â”‚  âœ… DONE          â”‚
â”‚  â”‚  â€¢ AudioRecord streaming (16kHz)       â”‚                    â”‚
â”‚  â”‚  â€¢ RMS amplitude calculation            â”‚                    â”‚
â”‚  â”‚  â€¢ Speech/silence detection             â”‚                    â”‚
â”‚  â”‚  â€¢ StateFlow<Boolean> isUserSpeaking   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  InterruptLogic.kt                     â”‚  âœ… DONE          â”‚
â”‚  â”‚  â€¢ Monitors user + AI speaking states  â”‚                    â”‚
â”‚  â”‚  â€¢ Detects barge-in conflicts          â”‚                    â”‚
â”‚  â”‚  â€¢ Emits BargeInEvent                  â”‚                    â”‚
â”‚  â”‚  â€¢ Cuts AI immediately on interrupt    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  AudioRecorder.kt                      â”‚  ğŸ”¨ NEXT          â”‚
â”‚  â”‚  â€¢ Captures user audio                 â”‚                    â”‚
â”‚  â”‚  â€¢ Saves to WAV/PCM buffer             â”‚                    â”‚
â”‚  â”‚  â€¢ Triggered by VAD silence            â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  RunAnywhere SDK Integration           â”‚  ğŸ”¨ NEXT          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â”‚
â”‚  â”‚  â”‚  WhisperModel (STT)               â”‚ â”‚                    â”‚
â”‚  â”‚  â”‚  â€¢ User audio â†’ text transcript   â”‚ â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â”‚
â”‚  â”‚  â”‚  Llama/Qwen Model (LLM)           â”‚ â”‚                    â”‚
â”‚  â”‚  â”‚  â€¢ System prompt: Language Coach  â”‚ â”‚                    â”‚
â”‚  â”‚  â”‚  â€¢ Input: user transcript         â”‚ â”‚                    â”‚
â”‚  â”‚  â”‚  â€¢ Output: JSON feedback          â”‚ â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                    â”‚
â”‚  â”‚  â”‚  TTS Model (Text-to-Speech)       â”‚ â”‚                    â”‚
â”‚  â”‚  â”‚  â€¢ AI response â†’ audio output     â”‚ â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  VoiceConversationManager.kt           â”‚  ğŸ”¨ NEXT          â”‚
â”‚  â”‚  â€¢ Orchestrates full conversation flow â”‚                    â”‚
â”‚  â”‚  â€¢ Handles state machine               â”‚                    â”‚
â”‚  â”‚  â€¢ Connects all components             â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Production UI (Compose)               â”‚  ğŸ”œ LATER         â”‚
â”‚  â”‚  â€¢ Voice waveform visualization        â”‚                    â”‚
â”‚  â”‚  â€¢ Real-time feedback display          â”‚                    â”‚
â”‚  â”‚  â€¢ Pronunciation scoring               â”‚                    â”‚
â”‚  â”‚  â€¢ Session history                     â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Speaks â†’ VAD Detects â†’ AudioRecorder Captures â†’ Whisper STT
    â†“
Transcript Generated â†’ LLM Processes (with language coach prompt)
    â†“
JSON Feedback Created â†’ TTS Speaks â†’ Audio Output
    â†“
AI Speaking â†’ InterruptLogic Monitors â†’ User Can Interrupt Anytime
```

---

## ğŸš€ Phase-by-Phase Roadmap

---

### âš ï¸ PHASE 0: PROJECT CLEANUP & ALIGNMENT ğŸ§¹ **START HERE - CRITICAL**

**Goal**: Clean up unused files and establish clear project structure for AI agent execution

**Status**: Not Started  
**Priority**: CRITICAL (MUST DO BEFORE STARTING PHASE 4)  
**Estimated Time**: 30 minutes  
**Assigned To**: AI Agent (Automated)

---

#### Why This Phase Exists

Before proceeding with development, we must:

1. **Remove confusion**: Delete unused/placeholder files that could mislead the AI
2. **Clarify structure**: Ensure only active, necessary files remain
3. **Document inventory**: Create clear file manifest for AI reference
4. **Prevent errors**: Avoid AI working on wrong/outdated files

---

#### Task 0.1: Identify and Remove Unused Files

**Files to DELETE** (Unused/Redundant Documentation):

```bash
# These are implementation summaries - keep for reference but move to docs/
- REFACTORING_SUMMARY.md (Phase 1 complete - archive)
- VAD_IMPLEMENTATION_SUMMARY.md (Phase 2 complete - archive)
- INTERRUPT_LOGIC_IMPLEMENTATION_SUMMARY.md (Phase 3 complete - archive)
```

**Action**: Move to `docs/archive/` folder (don't delete, just archive)

**Files to KEEP** (Active Documentation):

```bash
âœ… PROJECT_ROADMAP.md (THIS FILE - master guide)
âœ… README.md (public-facing documentation)
âœ… RUNANYWHERE_SDK_COMPLETE_GUIDE.md (SDK reference)
âœ… VAD_TESTING_GUIDE.md (active testing guide)
âœ… INTERRUPT_LOGIC_TESTING_GUIDE.md (active testing guide)
```

---

#### Task 0.2: Clean Up Placeholder Code

**File**: `app/src/main/java/com/pitchslap/app/data/Models.kt`

**Current Status**: Contains only TODO comments and no real code

**Action**:

- Option A: Delete entirely (will recreate in Phase 5.2)
- Option B: Keep but add clear header explaining it's for Phase 5

**Recommended**: **DELETE** - Will recreate with proper data models in Phase 5.2

```bash
# Delete this file
rm app/src/main/java/com/pitchslap/app/data/Models.kt
# Will recreate as:
# - app/src/main/java/com/pitchslap/app/data/PronunciationFeedback.kt (Phase 5.2)
# - app/src/main/java/com/pitchslap/app/data/ConversationTurn.kt (Phase 6.1)
```

---

#### Task 0.3: Create Project File Inventory

**Create**: `PROJECT_FILE_INVENTORY.md` (AI reference document)

**Purpose**: Clear map of all active files and their purpose

**Content**:

```markdown
# Project File Inventory

## ğŸ“‚ Active Source Files (DO USE)

### Core Application
- `app/src/main/java/com/pitchslap/app/PitchSlapApplication.kt`
  - Status: âœ… Complete
  - Purpose: SDK initialization, model registration
  - Phase: 1
  - Modify: Only for adding new models

- `app/src/main/java/com/pitchslap/app/MainActivity.kt`
  - Status: âœ… Complete (Test UI)
  - Purpose: Test interface for VAD + Interrupt Logic
  - Phase: 3
  - Modify: Will replace with production UI in Phase 7

### Voice Logic (Complete)
- `app/src/main/java/com/pitchslap/app/logic/VoiceActivityDetection.kt`
  - Status: âœ… Complete
  - Purpose: Real-time speech detection
  - Phase: 2
  - Modify: NO (unless bugs found)

- `app/src/main/java/com/pitchslap/app/logic/InterruptLogic.kt`
  - Status: âœ… Complete
  - Purpose: Barge-in detection
  - Phase: 3
  - Modify: NO (unless bugs found)

### UI Theme
- `app/src/main/java/com/pitchslap/app/ui/theme/*.kt`
  - Status: âœ… Complete
  - Purpose: Material 3 theme configuration
  - Phase: 1
  - Modify: Optional (customize colors)

---

## ğŸ“‚ To Be Created (Phase 4-8)

### Phase 4: Audio Recording
- âŒ `app/src/main/java/com/pitchslap/app/audio/AudioRecorder.kt`
  - Purpose: Capture user speech audio
  - Depends on: VoiceActivityDetection.kt

### Phase 5: AI Integration
- âŒ `app/src/main/java/com/pitchslap/app/ai/WhisperService.kt`
  - Purpose: Speech-to-text transcription
  - Depends on: AudioRecorder.kt

- âŒ `app/src/main/java/com/pitchslap/app/prompts/LanguageCoachPrompts.kt`
  - Purpose: System prompts and JSON schema
  - Depends on: Nothing (standalone)

- âŒ `app/src/main/java/com/pitchslap/app/data/PronunciationFeedback.kt`
  - Purpose: Data model for AI feedback
  - Depends on: LanguageCoachPrompts.kt

- âŒ `app/src/main/java/com/pitchslap/app/ai/FeedbackGenerator.kt`
  - Purpose: LLM integration for feedback
  - Depends on: PronunciationFeedback.kt, LanguageCoachPrompts.kt

- âŒ `app/src/main/java/com/pitchslap/app/audio/TextToSpeechEngine.kt`
  - Purpose: AI voice output
  - Depends on: InterruptLogic.kt

### Phase 6: Conversation Manager
- âŒ `app/src/main/java/com/pitchslap/app/conversation/VoiceConversationManager.kt`
  - Purpose: Orchestrates full voice pipeline
  - Depends on: ALL above components

- âŒ `app/src/main/java/com/pitchslap/app/data/ConversationTurn.kt`
  - Purpose: Data model for conversation history
  - Depends on: PronunciationFeedback.kt

### Phase 7: Production UI
- âŒ `app/src/main/java/com/pitchslap/app/ui/screens/*.kt`
  - Purpose: Production user interface
  - Depends on: VoiceConversationManager.kt

- âŒ `app/src/main/java/com/pitchslap/app/navigation/NavGraph.kt`
  - Purpose: Screen navigation
  - Depends on: UI screens

- âŒ `app/src/main/java/com/pitchslap/app/data/SessionRepository.kt`
  - Purpose: Database access for history
  - Depends on: Room setup

---

## ğŸ“‚ Configuration Files (DO NOT MODIFY)

- `app/build.gradle.kts` - Build configuration
- `gradle/libs.versions.toml` - Dependency versions
- `settings.gradle.kts` - Project settings
- `app/src/main/AndroidManifest.xml` - App manifest
- `local.properties` - Local paths (gitignored)

---

## ğŸ“‚ Documentation (REFERENCE ONLY)

### Active Guides
- `PROJECT_ROADMAP.md` - Master execution plan (THIS IS THE MAIN GUIDE)
- `README.md` - Public documentation
- `RUNANYWHERE_SDK_COMPLETE_GUIDE.md` - SDK API reference
- `VAD_TESTING_GUIDE.md` - Testing VAD
- `INTERRUPT_LOGIC_TESTING_GUIDE.md` - Testing interrupts

### Archived (Reference Only)
- `docs/archive/REFACTORING_SUMMARY.md`
- `docs/archive/VAD_IMPLEMENTATION_SUMMARY.md`
- `docs/archive/INTERRUPT_LOGIC_IMPLEMENTATION_SUMMARY.md`

---

## ğŸ¯ AI Agent Execution Rules

1. **ALWAYS** check this inventory before creating files
2. **NEVER** modify files marked "Modify: NO"
3. **FOLLOW** the phase sequence strictly (4 â†’ 5 â†’ 6 â†’ 7 â†’ 8)
4. **REFERENCE** the main PROJECT_ROADMAP.md for detailed instructions
5. **TEST** each component before moving to next phase
6. **LOG** progress in commit messages

---

Last Updated: [Current Date]
```

---

#### Task 0.4: Validate Project Structure

**Run These Checks**:

```bash
# 1. Verify all source files compile
./gradlew clean build

# 2. Check for unused imports
# (Android Studio: Code â†’ Optimize Imports)

# 3. Verify no duplicate files
find app/src -type f -name "*.kt" | sort

# 4. Check AndroidManifest permissions
grep "permission" app/src/main/AndroidManifest.xml
```

**Expected Results**:

- âœ… Build succeeds
- âœ… No compilation errors
- âœ… Only necessary permissions listed
- âœ… No duplicate file names

---

#### Task 0.5: Create AI Agent Execution Checklist

**Create**: `AI_EXECUTION_CHECKLIST.md`

**Purpose**: Step-by-step checklist for AI to follow

**Content**:

```markdown
# AI Agent Execution Checklist

## Before Starting Each Phase

- [ ] Read `PROJECT_ROADMAP.md` section for current phase
- [ ] Check `PROJECT_FILE_INVENTORY.md` for file locations
- [ ] Verify dependencies from previous phases are complete
- [ ] Review relevant testing guides if available

## During Phase Execution

- [ ] Create files in correct package structure
- [ ] Follow naming conventions (PascalCase for classes)
- [ ] Add KDoc comments to all public functions
- [ ] Use proper imports (no wildcards)
- [ ] Handle errors gracefully (try-catch)
- [ ] Log important events (android.util.Log)
- [ ] Test incrementally (don't write all code at once)

## After Completing Each Task

- [ ] Verify code compiles (./gradlew build)
- [ ] Run on device/emulator (if applicable)
- [ ] Check Logcat for errors
- [ ] Update phase status in PROJECT_ROADMAP.md
- [ ] Commit with clear message
- [ ] Update this checklist

## Phase Status Tracking

### Phase 0: Cleanup â¬œ
- [ ] Task 0.1: Archive old summaries
- [ ] Task 0.2: Delete placeholder Models.kt
- [ ] Task 0.3: Create file inventory
- [ ] Task 0.4: Validate structure
- [ ] Task 0.5: Create this checklist

### Phase 4: Audio Recording â¬œ
- [ ] Task 4.1: Create AudioRecorder.kt
- [ ] Task 4.2: Test audio recording

### Phase 5: SDK Integration â¬œ
- [ ] Task 5.1: Integrate Whisper
- [ ] Task 5.2: Design prompts
- [ ] Task 5.3: Integrate LLM
- [ ] Task 5.4: Integrate TTS

### Phase 6: Conversation Manager â¬œ
- [ ] Task 6.1: Create VoiceConversationManager
- [ ] Task 6.2: Update test UI

### Phase 7: Production UI â¬œ
- [ ] Task 7.1: Design screens
- [ ] Task 7.2: Progress tracking
- [ ] Task 7.3: Polish & animations

### Phase 8: Documentation â¬œ
- [ ] Task 8.1: Testing
- [ ] Task 8.2: Demo video
- [ ] Task 8.3: README
- [ ] Task 8.4: Submission

---

## Common Issues & Solutions

### Issue: "Package does not exist"
**Solution**: Check import statements, ensure file in correct package

### Issue: "Unresolved reference"
**Solution**: Check dependencies in build.gradle.kts, sync Gradle

### Issue: "Cannot access class"
**Solution**: Ensure class is public, not internal/private

### Issue: "StateFlow not collecting in UI"
**Solution**: Use .collectAsState() in Composables

### Issue: "Coroutine not running"
**Solution**: Check CoroutineScope is active, use proper Dispatcher

---

Last Updated: [Current Date]
```

---

#### Success Criteria for Phase 0

- [ ] Old summary files moved to `docs/archive/`
- [ ] Placeholder `Models.kt` deleted
- [ ] `PROJECT_FILE_INVENTORY.md` created
- [ ] `AI_EXECUTION_CHECKLIST.md` created
- [ ] Project structure validated (clean build)
- [ ] All documentation is up-to-date
- [ ] Ready to proceed to Phase 4

---

#### Deliverables

1. âœ… Clean project structure
2. âœ… `docs/archive/` folder with old summaries
3. âœ… `PROJECT_FILE_INVENTORY.md` (AI reference)
4. âœ… `AI_EXECUTION_CHECKLIST.md` (Step tracker)
5. âœ… Verified clean build

---

**âš ï¸ IMPORTANT**: This phase must be completed before starting Phase 4. It ensures the AI agent has a clean, organized
workspace and clear instructions for execution.

---

### Phase 4: Audio Recording & Buffering ğŸ”¨ NEXT AFTER PHASE 0

**Goal**: Capture user audio when they finish speaking for transcription

**Status**: Not Started  
**Priority**: HIGH  
**Estimated Time**: 4-6 hours

#### Tasks

**4.1: Create AudioRecorder Class**

**File**: `app/src/main/java/com/pitchslap/app/logic/AudioRecorder.kt`

**Requirements**:

- Use `AudioRecord` API (same as VAD)
- Record audio at **16kHz, 16-bit PCM, Mono** (Whisper requirement)
- Store audio in circular buffer during VAD speaking = true
- When VAD speaking = false (silence detected), save buffer and emit event
- Expose `Flow<AudioBuffer>` with recorded audio chunks

**Key API**:

```kotlin
class AudioRecorder {
    // Start recording when user starts speaking
    fun startRecording()
    
    // Get recorded audio buffer when user stops
    suspend fun getAudioBuffer(): ByteArray
    
    // Save to WAV file (for Whisper compatibility)
    suspend fun saveToWAV(buffer: ByteArray, outputFile: File)
    
    // Clear buffer
    fun clearBuffer()
}
```

**Technical Details**:

- Buffer Size: ~3 seconds of audio (16000 samples/sec * 2 bytes * 3 sec = 96KB)
- Format: PCM 16-bit signed
- Output: WAV file or raw ByteArray

**Integration Point**:

```kotlin
// In VoiceConversationManager
vad.isUserSpeaking.collect { speaking ->
    if (speaking) {
        audioRecorder.startRecording()
    } else {
        // User finished speaking
        val audioData = audioRecorder.getAudioBuffer()
        sendToWhisper(audioData)
    }
}
```

**Success Criteria**:

- [ ] Audio recorded during speech
- [ ] WAV file saved correctly
- [ ] Audio playable in media player
- [ ] Buffer management works (no overflow)
- [ ] Integrates with VAD seamlessly

**Deliverable**: `AudioRecorder.kt` (~150 lines)

---

**4.2: Test Audio Recording**

**File**: Update `MainActivity.kt` with test UI

**Requirements**:

- Add "Record Audio" button
- Show recording indicator when VAD detects speech
- Save audio to file when speech ends
- Add "Play Recording" button to verify
- Display audio file size/duration

**Success Criteria**:

- [ ] Can record and play back audio
- [ ] Audio quality is good
- [ ] No distortion or clipping
- [ ] File size is reasonable

---

### Phase 5: RunAnywhere SDK Integration ğŸ”¨ NEXT

**Goal**: Connect voice pipeline to on-device AI models

**Status**: Not Started  
**Priority**: HIGH  
**Estimated Time**: 8-12 hours

#### Tasks

**5.1: Integrate Whisper Model (Speech-to-Text)**

**File**: Update `PitchSlapApplication.kt` + create `WhisperService.kt`

**Requirements**:

*Note: Check if RunAnywhere SDK supports Whisper. If not, use alternative STT:*

- **Option A**: RunAnywhere Whisper (if available)
- **Option B**: Vosk (lightweight offline STT)
- **Option C**: Android SpeechRecognizer (online, fallback)

**For RunAnywhere Whisper** (preferred):

```kotlin
// In PitchSlapApplication.kt
suspend fun registerModels() {
    // Add Whisper model
    addModelFromURL(
        url = "https://huggingface.co/.../whisper-tiny.gguf",
        name = "Whisper Tiny",
        type = "STT"
    )
    
    // Add LLM model for feedback
    addModelFromURL(
        url = "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q6_k.gguf",
        name = "Qwen 2.5 1.5B Instruct Q6_K",
        type = "LLM"
    )
}
```

**WhisperService API**:

```kotlin
class WhisperService {
    // Transcribe audio file to text
    suspend fun transcribe(audioFile: File): String
    
    // Stream transcription (if supported)
    fun transcribeStream(audioFile: File): Flow<String>
}
```

**Success Criteria**:

- [ ] Whisper model downloads successfully
- [ ] Audio transcription works
- [ ] Latency < 500ms for 3-second audio
- [ ] Accuracy is acceptable for English

**Deliverable**: `WhisperService.kt` (~100 lines)

---

**5.2: Design Language Coach System Prompt**

**File**: `app/src/main/java/com/pitchslap/app/prompts/LanguageCoachPrompts.kt`

**Requirements**:

- Create system prompt for language coaching personality
- Define JSON output format for pronunciation feedback
- Include few-shot examples for consistency

**System Prompt Structure**:

```kotlin
object LanguageCoachPrompts {
    
    val SYSTEM_PROMPT = """
        You are an expert pronunciation coach for English language learners.
        You provide instant, encouraging feedback on pronunciation, grammar, and fluency.
        
        For each user input, respond with JSON in this exact format:
        {
            "transcript": "what the user said",
            "pronunciation_score": 0-100,
            "grammar_score": 0-100,
            "fluency_score": 0-100,
            "feedback": "brief encouraging feedback",
            "corrections": ["specific pronunciation tips"],
            "example_sentence": "a practice sentence for improvement"
        }
        
        Be encouraging and specific. Focus on 1-2 key improvements per response.
    """.trimIndent()
    
    fun createPrompt(userTranscript: String, conversationHistory: List<String> = emptyList()): String {
        return buildString {
            appendLine(SYSTEM_PROMPT)
            appendLine()
            
            // Add conversation history if available
            conversationHistory.forEach { historyItem ->
                appendLine(historyItem)
            }
            
            appendLine("User said: \"$userTranscript\"")
            appendLine()
            appendLine("Provide JSON feedback:")
        }
    }
}
```

**JSON Schema** (for structured output):

```kotlin
@Serializable
data class PronunciationFeedback(
    val transcript: String,
    val pronunciation_score: Int,
    val grammar_score: Int,
    val fluency_score: Int,
    val feedback: String,
    val corrections: List<String>,
    val example_sentence: String
)
```

**Success Criteria**:

- [ ] System prompt is clear and effective
- [ ] JSON output is consistent
- [ ] Feedback is encouraging and actionable
- [ ] Scoring makes sense (0-100 scale)

**Deliverable**: `LanguageCoachPrompts.kt` (~80 lines)

---

**5.3: Integrate LLM for Feedback Generation**

**File**: `app/src/main/java/com/pitchslap/app/ai/FeedbackGenerator.kt`

**Requirements**:

- Load Qwen/Llama model
- Send user transcript with language coach prompt
- Parse JSON response
- Handle generation errors gracefully
- Implement streaming for real-time feedback

**API Design**:

```kotlin
class FeedbackGenerator {
    
    // Initialize and load model
    suspend fun initialize(): Boolean
    
    // Generate feedback from transcript (blocking)
    suspend fun generateFeedback(transcript: String): Result<PronunciationFeedback>
    
    // Generate feedback with streaming (for real-time display)
    fun generateFeedbackStream(transcript: String): Flow<String>
    
    // Parse JSON response into data class
    private fun parseResponse(jsonString: String): PronunciationFeedback
}
```

**Implementation Notes**:

- Use `RunAnywhere.loadModel()` to load LLM
- Use `RunAnywhere.generate()` for blocking generation
- Use `RunAnywhere.generateStream()` for streaming
- Parse JSON using `kotlinx.serialization`
- Handle malformed JSON gracefully

**Error Handling**:

- Model not loaded â†’ retry or show error
- Generation timeout â†’ cancel and retry
- Invalid JSON â†’ use default feedback
- Barge-in during generation â†’ cancel immediately

**Success Criteria**:

- [ ] Model loads successfully
- [ ] Generates valid JSON feedback
- [ ] Latency < 2 seconds for response
- [ ] Handles errors without crashing
- [ ] Respects barge-in interrupts

**Deliverable**: `FeedbackGenerator.kt` (~200 lines)

---

**5.4: Integrate TTS (Text-to-Speech)**

**File**: `app/src/main/java/com/pitchslap/app/audio/TextToSpeechEngine.kt`

**Options for TTS**:

**Option A**: RunAnywhere TTS (if available)
**Option B**: Android native TTS (good quality, reliable)
**Option C**: Festival/eSpeak (offline, lower quality)

**Recommended**: Use Android native TTS (best UX, built-in)

**API Design**:

```kotlin
class TextToSpeechEngine(context: Context) {
    
    // Initialize TTS engine
    fun initialize(onReady: () -> Unit)
    
    // Speak text with interruption support
    fun speak(
        text: String,
        onStart: () -> Unit = {},
        onDone: () -> Unit = {},
        onError: (String) -> Unit = {}
    )
    
    // Stop speaking immediately (for barge-in)
    fun stop()
    
    // Cleanup
    fun shutdown()
}
```

**Integration with InterruptLogic**:

```kotlin
// Mark AI as speaking when TTS starts
tts.speak(
    text = feedback.feedback,
    onStart = { interruptLogic.startAiSpeech() },
    onDone = { interruptLogic.stopAiSpeech() }
)

// Listen for barge-ins
interruptLogic.bargeInEvent.collect { event ->
    // Stop TTS immediately when user interrupts
    tts.stop()
}
```

**Success Criteria**:

- [ ] TTS initializes successfully
- [ ] Speech quality is clear
- [ ] Latency < 200ms to start speaking
- [ ] Can interrupt mid-speech
- [ ] No audio glitches

**Deliverable**: `TextToSpeechEngine.kt` (~150 lines)

---

### Phase 6: Voice Conversation Manager ğŸ”œ AFTER PHASE 5

**Goal**: Orchestrate the full conversation loop

**Status**: Not Started  
**Priority**: HIGH  
**Estimated Time**: 6-8 hours

#### Tasks

**6.1: Create VoiceConversationManager**

**File**: `app/src/main/java/com/pitchslap/app/conversation/VoiceConversationManager.kt`

**Requirements**:

- State machine for conversation flow
- Coordinates VAD, AudioRecorder, Whisper, LLM, TTS, InterruptLogic
- Manages conversation history
- Handles all error cases
- Exposes UI-reactive states

**State Machine**:

```
IDLE â†’ LISTENING â†’ PROCESSING â†’ AI_SPEAKING â†’ IDLE
  â†‘                                              â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           (Barge-in can force any state â†’ LISTENING)
```

**API Design**:

```kotlin
class VoiceConversationManager(
    private val vad: VoiceActivityDetection,
    private val audioRecorder: AudioRecorder,
    private val whisperService: WhisperService,
    private val feedbackGenerator: FeedbackGenerator,
    private val tts: TextToSpeechEngine,
    private val interruptLogic: InterruptLogic
) {
    
    // Conversation state
    enum class ConversationState {
        IDLE,           // Not active
        LISTENING,      // VAD active, waiting for user speech
        RECORDING,      // User speaking
        PROCESSING,     // Transcribing + generating feedback
        AI_SPEAKING,    // AI providing feedback
        ERROR           // Error occurred
    }
    
    // Exposed state
    val conversationState: StateFlow<ConversationState>
    val currentFeedback: StateFlow<PronunciationFeedback?>
    val conversationHistory: StateFlow<List<ConversationTurn>>
    
    // Control methods
    suspend fun startConversation()
    fun stopConversation()
    fun clearHistory()
    
    // Internal orchestration
    private suspend fun conversationLoop()
    private suspend fun handleUserSpeech(audioBuffer: ByteArray)
    private suspend fun handleAIResponse(feedback: PronunciationFeedback)
    private suspend fun handleBargeIn(event: BargeInEvent)
}
```

**Conversation Loop Logic**:

```kotlin
private suspend fun conversationLoop() {
    // Start VAD
    vad.start()
    
    while (isActive) {
        // Wait for user to speak
        vad.isUserSpeaking.collect { speaking ->
            when {
                speaking -> {
                    // User started speaking
                    _conversationState.value = ConversationState.RECORDING
                    audioRecorder.startRecording()
                }
                
                !speaking && _conversationState.value == ConversationState.RECORDING -> {
                    // User stopped speaking
                    _conversationState.value = ConversationState.PROCESSING
                    
                    // Get audio
                    val audio = audioRecorder.getAudioBuffer()
                    
                    // Transcribe
                    val transcript = whisperService.transcribe(audio)
                    
                    // Generate feedback
                    val feedback = feedbackGenerator.generateFeedback(transcript).getOrNull()
                    
                    if (feedback != null) {
                        // Speak feedback
                        _conversationState.value = ConversationState.AI_SPEAKING
                        handleAIResponse(feedback)
                    }
                    
                    // Return to listening
                    _conversationState.value = ConversationState.LISTENING
                }
            }
        }
        
        // Monitor for barge-ins
        interruptLogic.bargeInEvent.collect { event ->
            handleBargeIn(event)
        }
    }
}
```

**Success Criteria**:

- [ ] Full conversation loop works end-to-end
- [ ] State transitions are clean
- [ ] Barge-in interrupts work correctly
- [ ] Error handling is robust
- [ ] No memory leaks

**Deliverable**: `VoiceConversationManager.kt` (~300 lines)

---

**6.2: Update Test UI**

**File**: Update `MainActivity.kt`

**Requirements**:

- Replace interrupt test UI with conversation test UI
- Show conversation state visually
- Display real-time feedback scores
- Show conversation history
- Add "Start Practice" / "Stop Practice" buttons

**UI Elements**:

- Large state indicator (IDLE/LISTENING/PROCESSING/AI_SPEAKING)
- Real-time waveform visualization (optional, nice-to-have)
- Transcript display
- Feedback scores (pronunciation/grammar/fluency)
- Correction tips list
- Example sentence display
- Conversation history (last 5 turns)

**Success Criteria**:

- [ ] UI reflects conversation state accurately
- [ ] Feedback is displayed clearly
- [ ] Easy to start/stop practice session
- [ ] Responsive and smooth

**Deliverable**: Updated `MainActivity.kt` (~400 lines)

---

### Phase 7: Production UI & Polish ğŸ”œ FINAL PHASE

**Goal**: Create polished, professional user interface

**Status**: Not Started  
**Priority**: MEDIUM  
**Estimated Time**: 8-10 hours

#### Tasks

**7.1: Design Production UI**

**Screens**:

1. **Home Screen**
    - Welcome message
    - "Start Practice" button
    - Settings icon
    - Session history

2. **Practice Screen** (Main)
    - Large microphone button (visual feedback)
    - Voice waveform visualization
    - Real-time transcript
    - Feedback card (animated entrance)
    - Score badges (pronunciation/grammar/fluency)
    - "Next" button to continue

3. **Feedback Detail Screen**
    - Detailed breakdown of scores
    - Correction tips with examples
    - Practice sentence with audio playback
    - "Try Again" button

4. **History Screen**
    - List of past sessions
    - Progress chart
    - Stats (avg scores, total practice time)

5. **Settings Screen**
    - Model selection
    - Download models
    - Language selection (if multi-language)
    - Reset progress

**Design Principles**:

- Material 3 design system
- Large, touch-friendly buttons
- Clear visual feedback for all states
- Smooth animations
- Accessible (high contrast, readable fonts)

**Success Criteria**:

- [ ] UI is visually appealing
- [ ] Navigation is intuitive
- [ ] Animations are smooth
- [ ] Responsive on different screen sizes
- [ ] Follows Material Design guidelines

**Deliverable**:

- `app/src/main/java/com/pitchslap/app/ui/screens/` (multiple files, ~800 lines total)
- `app/src/main/java/com/pitchslap/app/navigation/NavGraph.kt`

---

**7.2: Implement Progress Tracking**

**File**: `app/src/main/java/com/pitchslap/app/data/SessionRepository.kt`

**Requirements**:

- Use Room database to store session history
- Track scores over time
- Calculate progress metrics
- Expose as Flow for reactive UI

**Data Model**:

```kotlin
@Entity
data class PracticeSession(
    @PrimaryKey(autoGenerate = true) val id: Long = 0,
    val timestamp: Long,
    val transcript: String,
    val pronunciationScore: Int,
    val grammarScore: Int,
    val fluencyScore: Int,
    val feedback: String,
    val corrections: List<String>,
    val durationMs: Long
)
```

**Success Criteria**:

- [ ] Sessions saved to database
- [ ] History screen shows past sessions
- [ ] Progress chart displays improvement
- [ ] No database errors

**Deliverable**:

- `SessionRepository.kt` (~150 lines)
- `SessionDao.kt` (~50 lines)
- `AppDatabase.kt` (~80 lines)

---

**7.3: Add Polish & Animation**

**Enhancements**:

- Smooth transitions between screens
- Animated score counters
- Voice waveform visualization
- Haptic feedback on key actions
- Success celebrations (confetti for high scores)
- Error animations (shake for low scores)

**Libraries to Consider**:

- Lottie for animations
- MPAndroidChart for progress graphs
- Accompanist for Compose utilities

**Success Criteria**:

- [ ] App feels polished and professional
- [ ] Animations are smooth (60fps)
- [ ] User experience is delightful
- [ ] No performance issues

**Deliverable**: Updated UI files with animations

---

### Phase 8: Testing & Documentation ğŸ”œ FINAL WEEK

**Goal**: Ensure quality and prepare deliverables

**Status**: Not Started  
**Priority**: HIGH  
**Estimated Time**: 6-8 hours

#### Tasks

**8.1: Comprehensive Testing**

**Test Categories**:

1. **Functional Testing**
    - [ ] End-to-end conversation flow works
    - [ ] All buttons and controls work
    - [ ] Audio recording/playback works
    - [ ] Model download/loading works
    - [ ] Feedback generation is accurate
    - [ ] Barge-in interrupts work reliably

2. **Performance Testing**
    - [ ] Latency < 80ms for VAD detection
    - [ ] Transcription < 500ms
    - [ ] Feedback generation < 2s
    - [ ] No UI lag or stuttering
    - [ ] Memory usage reasonable (<400MB)

3. **Device Testing**
    - [ ] Test on high-end device (8GB+ RAM)
    - [ ] Test on mid-range device (4GB RAM)
    - [ ] Test on low-end device (2GB RAM)
    - [ ] Test in noisy environment
    - [ ] Test in quiet environment

4. **Edge Case Testing**
    - [ ] No internet connection (offline mode)
    - [ ] Low battery
    - [ ] Phone call interruption
    - [ ] App backgrounded/foregrounded
    - [ ] Screen rotation
    - [ ] Very long speech input
    - [ ] Gibberish/non-English input
    - [ ] Multiple rapid barge-ins

**Bug Tracking**: Use GitHub Issues or simple markdown checklist

**Success Criteria**:

- [ ] No critical bugs
- [ ] All features work as expected
- [ ] Performance meets targets
- [ ] Edge cases handled gracefully

---

**8.2: Create Demo Video** ğŸ¥

**Requirements**:

- **Duration**: 2-3 minutes
- **Quality**: 1080p or higher
- **Format**: MP4
- **Content**: Show actual device running app

**Video Structure**:

1. **Intro (15 seconds)**
    - Team name + project title
    - Problem statement (cloud latency)
    - Our solution (Pitch Slap)

2. **Demo (90 seconds)**
    - Show app startup
    - Start practice session
    - Speak a sentence (show real-time detection)
    - Display feedback with scores
    - Speak another sentence (show improvement)
    - Demonstrate barge-in (interrupt AI mid-speech)
    - Show conversation history

3. **Technical Highlights (30 seconds)**
    - On-device processing (no internet)
    - Sub-80ms latency
    - Model information
    - Architecture diagram (quick flash)

4. **Outro (15 seconds)**
    - Key benefits recap
    - Team members (optional)
    - Thank you

**Recording Tips**:

- Use screen recording on device (adb screenrecord or built-in)
- Record in quiet environment with good lighting
- Use voiceover to explain what's happening
- Add text overlays for clarity
- Include latency numbers if possible
- Show Logcat/debug info briefly (proves it's on-device)

**Tools**:

- Screen recording: `adb shell screenrecord /sdcard/demo.mp4`
- Video editing: iMovie, DaVinci Resolve, or Shotcut (free)
- Voiceover: Audacity or QuickTime
- Text overlays: Any video editor

**Success Criteria**:

- [ ] Video is clear and professional
- [ ] Demonstrates all key features
- [ ] Shows on-device processing
- [ ] Highlights low latency
- [ ] Under 3 minutes
- [ ] Submitted in correct format

**Deliverable**: `demo_video.mp4` (uploaded to GitHub or YouTube)

---

**8.3: Write Comprehensive README.md** ğŸ“–

**Requirements**: Professional documentation for hackathon judges

**README Structure**:

```markdown
# Pitch Slap - Zero-Latency Language Practice Partner

**Hackathon**: RunAnywhere AI x Firebender Challenge
**Track**: PS 3 - The Zero-Latency Voice Interface
**Team**: [Your Team Name]

## ğŸ¯ Problem Statement

Traditional cloud-based voice assistants suffer from latency issues that break 
the immersion of natural conversation. Language learners need instant feedback 
to correct pronunciation in real-time, just like speaking with a native speaker.

## ğŸ’¡ Our Solution

Pitch Slap delivers sub-80ms latency pronunciation feedback using 100% on-device AI.
No cloud required, no internet needed, complete privacy guaranteed.

## âœ¨ Key Features

- âš¡ **Sub-80ms Latency**: Instant speech detection and response
- ğŸ”’ **Complete Privacy**: All processing happens on your device
- ğŸ’° **Zero Cost**: No API fees or cloud inference charges
- ğŸ“¡ **Fully Offline**: Works without internet after model download
- ğŸ—£ï¸ **Natural Interruption**: Barge-in support for fluid conversations
- ğŸ“Š **Real-time Feedback**: Pronunciation, grammar, and fluency scores

## ğŸ—ï¸ Technical Architecture

[Insert architecture diagram]

### Components

1. **Voice Activity Detection (VAD)**
   - Real-time speech detection using RMS amplitude analysis
   - 16kHz audio streaming with <20ms detection latency

2. **Interrupt Logic**
   - Smart turn-taking with barge-in support
   - Immediate AI cutoff when user speaks

3. **Audio Processing Pipeline**
   - On-device recording and buffering
   - WAV format for Whisper compatibility

4. **RunAnywhere SDK Integration**
   - Whisper Tiny for Speech-to-Text
   - Qwen 2.5 1.5B for feedback generation
   - Android TTS for voice output

## ğŸ¬ Demo Video

[Link to demo video]

## ğŸš€ How to Run

### Prerequisites
- Android device with 4GB+ RAM
- Android 7.0 (API 24) or higher
- ~2GB free storage for models

### Installation

1. Clone the repository:
```bash
git clone [your-repo-url]
cd pitch-slap
```

2. Open in Android Studio

3. Sync Gradle dependencies

4. Run on device:

```bash
./gradlew installDebug
adb shell am start -n com.pitchslap.app/.MainActivity
```

5. On first launch:
    - Grant microphone permission
    - Download models (automatic)
    - Wait for models to load (~30 seconds)
    - Start practicing!

## ğŸ“± Usage

1. **Start Practice**: Tap the microphone button
2. **Speak Clearly**: Say a sentence in English
3. **Get Feedback**: Receive instant pronunciation scores
4. **Improve**: Follow correction tips and practice again
5. **Track Progress**: View your improvement over time

## ğŸ”¬ Technical Implementation

### Models Used

- **STT**: Whisper Tiny (39MB) - Fast, accurate speech recognition
- **LLM**: Qwen 2.5 1.5B Instruct (1.2GB) - Language coaching and feedback
- **TTS**: Android native TextToSpeech - High-quality voice output

### RunAnywhere SDK Features Utilized

1. âœ… **On-Device Inference**: All models run locally
2. âœ… **Model Management**: Download and caching system
3. âœ… **Streaming Generation**: Real-time feedback display
4. âœ… **Structured Output**: JSON-formatted feedback
5. âœ… **Low Latency**: Optimized llama.cpp variants

### Performance Metrics

| Metric | Target | Achieved |
|--------|--------|----------|
| Speech Detection | <80ms | ~20ms âœ… |
| Transcription | <500ms | ~300ms âœ… |
| Feedback Generation | <2s | ~1.5s âœ… |
| Barge-in Response | <100ms | ~20ms âœ… |

## ğŸ† Why On-Device?

1. **Latency**: Cloud round-trip adds 200-500ms minimum
2. **Privacy**: User speech never leaves device
3. **Cost**: No API fees or rate limits
4. **Reliability**: Works offline, no server downtime
5. **Scalability**: Zero infrastructure costs

## ğŸ“Š Project Statistics

- **Total Code**: ~2000 lines of Kotlin
- **Development Time**: [Your timeline]
- **Team Size**: 3 developers
- **Models Used**: 2 AI models (Whisper + Qwen)
- **APK Size**: ~50MB (without models)

## ğŸ‘¥ Team

- [Member 1] - VAD & Audio Processing
- [Member 2] - AI Integration & Prompts
- [Member 3] - UI/UX & Testing

## ğŸ™ Acknowledgments

- RunAnywhere AI for the powerful on-device SDK
- Y Combinator for backing innovative AI technology
- Hackathon organizers for this amazing opportunity

## ğŸ“„ License

[Your license choice]

```

**Success Criteria**:
- [ ] README is comprehensive and professional
- [ ] All sections are complete
- [ ] Includes architecture diagram
- [ ] Has demo video link
- [ ] Explains on-device necessity clearly
- [ ] Highlights technical achievements

**Deliverable**: `README.md` (~300 lines)

---

**8.4: Prepare Submission Package**

**Final Checklist**:

1. **GitHub Repository**
   - [ ] All code committed and pushed
   - [ ] README.md complete
   - [ ] Clean commit history
   - [ ] No sensitive data (API keys, tokens)
   - [ ] Repository is public

2. **Demo Video**
   - [ ] Video recorded and edited
   - [ ] Uploaded to YouTube or GitHub
   - [ ] Link added to README

3. **Documentation**
   - [ ] All .md files updated
   - [ ] Code comments are clear
   - [ ] Architecture diagrams included
   - [ ] Testing guide available

4. **Code Quality**
   - [ ] No compilation errors
   - [ ] No linter warnings (or documented exceptions)
   - [ ] Code is well-organized
   - [ ] No unused code or TODOs

5. **Submission Form** (Unstop)
   - [ ] GitHub repository link
   - [ ] Demo video link
   - [ ] Team member details
   - [ ] Problem statement selection
   - [ ] Brief project description

**Success Criteria**:
- [ ] Submission is complete
- [ ] All deliverables are professional
- [ ] Repository is accessible
- [ ] Video is clear and compelling

---

## ğŸ‘¥ Team Task Distribution

### Team Member 1: Audio & Voice Pipeline Expert

**Focus Areas**: VAD, Audio Recording, Integration

**Phases**:
- âœ… Phase 2: VAD Implementation (Already complete!)
- âœ… Phase 3: Interrupt Logic (Already complete!)
- ğŸ”¨ **Phase 4: Audio Recording** (Current)
  - Task 4.1: Create AudioRecorder class
  - Task 4.2: Test audio recording
- ğŸ”¨ **Phase 6: Conversation Manager** (Support)
  - Help integrate audio components
  - Debug audio-related issues

**Estimated Time**: 8-10 hours remaining

**Deliverables**:
- `AudioRecorder.kt`
- Audio recording test UI
- Integration support

---

### Team Member 2: AI Integration Specialist

**Focus Areas**: RunAnywhere SDK, Models, Prompts

**Phases**:
- ğŸ”¨ **Phase 5: SDK Integration** (Primary)
  - Task 5.1: Integrate Whisper (STT)
  - Task 5.2: Design system prompts
  - Task 5.3: Integrate LLM for feedback
  - Task 5.4: Integrate TTS
- ğŸ”¨ **Phase 6: Conversation Manager** (Lead)
  - Task 6.1: Create VoiceConversationManager
  - Orchestrate all components

**Estimated Time**: 14-18 hours

**Deliverables**:
- `WhisperService.kt`
- `LanguageCoachPrompts.kt`
- `FeedbackGenerator.kt`
- `TextToSpeechEngine.kt`
- `VoiceConversationManager.kt`

---

### Team Member 3: UI/UX & Documentation Lead

**Focus Areas**: User Interface, Testing, Documentation

**Phases**:
- ğŸ”¨ **Phase 6: Test UI** (Support)
  - Task 6.2: Update MainActivity with conversation test UI
- ğŸ”¨ **Phase 7: Production UI** (Lead)
  - Task 7.1: Design and implement production screens
  - Task 7.2: Implement progress tracking (Room DB)
  - Task 7.3: Add polish and animations
- ğŸ”¨ **Phase 8: Testing & Documentation** (Lead)
  - Task 8.1: Comprehensive testing
  - Task 8.2: Create demo video
  - Task 8.3: Write README.md
  - Task 8.4: Prepare submission

**Estimated Time**: 18-22 hours

**Deliverables**:
- Complete UI redesign (all screens)
- `SessionRepository.kt`, `SessionDao.kt`, `AppDatabase.kt`
- Demo video (2-3 minutes)
- Professional README.md
- Testing report
- Submission package

---

### Coordination Points

**Daily Sync** (Recommended):
- Quick 15-minute standup
- What did you accomplish?
- What are you working on today?
- Any blockers?

**Integration Points** (Critical handoffs):

1. **Member 1 â†’ Member 2**: AudioRecorder â†’ WhisperService
   - Coordinate audio format (WAV vs raw PCM)
   - Test audio file compatibility

2. **Member 2 â†’ Member 3**: VoiceConversationManager â†’ UI
   - Define StateFlow APIs for UI observation
   - Test conversation state transitions

3. **All Members**: Testing Phase
   - Test together on actual devices
   - Document bugs and fixes
   - Review each other's code

**Code Review**: Each merge to main should be reviewed by at least one other member

**Branch Strategy**:
- `main` - stable, working code
- `feature/audio-recording` - Member 1
- `feature/ai-integration` - Member 2
- `feature/production-ui` - Member 3

---

## ğŸ“‹ Final Deliverables Checklist

### Must-Have (Required for Submission)

- [ ] **GitHub Repository** (public)
  - [ ] Complete source code
  - [ ] README.md with all sections
  - [ ] Clean commit history
  - [ ] No sensitive data

- [ ] **Demo Video** (2-3 minutes)
  - [ ] Shows app running on device
  - [ ] Demonstrates key features
  - [ ] Highlights on-device processing
  - [ ] Professional quality

- [ ] **Project Documentation**
  - [ ] Problem statement explained
  - [ ] Models used listed
  - [ ] RunAnywhere SDK features detailed
  - [ ] Architecture described

- [ ] **Working Application**
  - [ ] Compiles without errors
  - [ ] Runs on Android device
  - [ ] Core features functional
  - [ ] No critical bugs

### Nice-to-Have (Bonus Points)

- [ ] Architecture diagrams
- [ ] Performance benchmarks
- [ ] Unit tests
- [ ] UI/UX polish (animations)
- [ ] Progress tracking feature
- [ ] Multiple model support
- [ ] Detailed testing report
- [ ] Contribution guide

---

## âœ… Testing & Quality Assurance

### Testing Strategy

**1. Unit Testing** (Optional but recommended)
- VAD amplitude calculation
- Audio buffer management
- JSON parsing
- State machine transitions

**2. Integration Testing** (Critical)
- VAD â†’ AudioRecorder handoff
- AudioRecorder â†’ Whisper transcription
- Whisper â†’ LLM â†’ TTS pipeline
- InterruptLogic cancellation

**3. End-to-End Testing** (Most Important)
- Full conversation flow
- Barge-in scenarios
- Error recovery
- Different speech patterns

**4. Device Testing** (Critical)
- Test on 3+ different devices
- Test with real users (not just developers)
- Test in different environments (quiet, noisy)
- Test edge cases (long speech, gibberish)

**5. Performance Testing**
- Measure latencies with Logcat timestamps
- Monitor memory usage (Android Profiler)
- Check battery drain
- Verify model loading times

### Test Cases

**Voice Activity Detection**:
- [ ] Detects normal speech
- [ ] Detects whisper
- [ ] Ignores background noise
- [ ] Handles sudden loud sounds
- [ ] Detects speech end correctly (500ms silence)

**Audio Recording**:
- [ ] Records full speech without clipping
- [ ] Handles long recordings (>30 seconds)
- [ ] Produces valid WAV files
- [ ] Audio quality is acceptable

**Speech-to-Text**:
- [ ] Transcribes clear speech accurately
- [ ] Handles accents reasonably
- [ ] Works in noisy environment (with degradation)
- [ ] Latency < 500ms

**Feedback Generation**:
- [ ] Generates valid JSON
- [ ] Scores are reasonable (0-100)
- [ ] Feedback is relevant
- [ ] Handles gibberish gracefully
- [ ] Latency < 2 seconds

**Text-to-Speech**:
- [ ] Voice is clear and natural
- [ ] Pronunciation is correct
- [ ] Can be interrupted mid-speech
- [ ] No audio glitches

**Barge-In**:
- [ ] Detects interrupt within 100ms
- [ ] Stops TTS immediately
- [ ] Cancels LLM generation
- [ ] Returns to listening state
- [ ] Works multiple times in a row

**UI/UX**:
- [ ] Responsive and smooth (60fps)
- [ ] Clear visual feedback for all states
- [ ] Error messages are helpful
- [ ] Navigation is intuitive
- [ ] Works on different screen sizes

**Edge Cases**:
- [ ] No internet connection
- [ ] Models not downloaded
- [ ] Low memory condition
- [ ] Phone call interruption
- [ ] App backgrounded during conversation
- [ ] Multiple rapid barge-ins

---

## â° Timeline & Milestones

### Recommended Schedule (Assuming 7 days)

**Day 1-2: Core Pipeline** (Member 1 + Member 2)
- Complete audio recording (Member 1)
- Integrate Whisper STT (Member 2)
- Test transcription end-to-end

**Day 3-4: AI Feedback** (Member 2 + Member 3)
- Design prompts and JSON schema (Member 2)
- Integrate LLM and TTS (Member 2)
- Create test UI for feedback display (Member 3)

**Day 5: Full Integration** (All Members)
- Build VoiceConversationManager (Member 2)
- Connect all components (All)
- End-to-end testing (All)
- Bug fixes (All)

**Day 6: Polish** (Member 3 + All)
- Implement production UI (Member 3)
- Add animations and polish (Member 3)
- More testing and bug fixes (All)
- Performance optimization (All)

**Day 7: Documentation & Submission** (Member 3 + All)
- Record demo video (Member 3 + All)
- Write README (Member 3)
- Final testing (All)
- Submit to Unstop (All)

### Milestones

| Milestone | Target Date | Status |
|-----------|-------------|--------|
| âœ… VAD Complete | [Completed] | DONE âœ… |
| âœ… Interrupt Logic Complete | [Completed] | DONE âœ… |
| ğŸ”¨ Audio Recording Complete | Day 2 | IN PROGRESS |
| ğŸ”¨ Whisper Integration Complete | Day 2 | PENDING |
| ğŸ”¨ LLM Integration Complete | Day 4 | PENDING |
| ğŸ”¨ Full Pipeline Working | Day 5 | PENDING |
| ğŸ”œ Production UI Complete | Day 6 | PENDING |
| ğŸ”œ Demo Video Complete | Day 7 | PENDING |
| ğŸ”œ Submission Ready | Day 7 | PENDING |

---

## ğŸ“ Key Learning Resources

### RunAnywhere SDK
- `RUNANYWHERE_SDK_COMPLETE_GUIDE.md` (in this repo)
- [Official Docs](https://github.com/RunanywhereAI/runanywhere-sdks)
- [SDK Repository](https://github.com/RunanywhereAI/runanywhere-sdks)

### Audio Processing
- [Android AudioRecord](https://developer.android.com/reference/android/media/AudioRecord)
- [PCM Audio Format](https://en.wikipedia.org/wiki/Pulse-code_modulation)
- [WAV File Format](http://soundfile.sapp.org/doc/WaveFormat/)

### AI Models
- [Whisper](https://huggingface.co/models?search=whisper)
- [Qwen Models](https://huggingface.co/Qwen)
- [GGUF Format](https://github.com/ggerganov/llama.cpp/blob/master/docs/gguf-format.md)

### Android Development
- [Jetpack Compose](https://developer.android.com/jetpack/compose)
- [Kotlin Flows](https://kotlinlang.org/docs/flow.html)
- [Room Database](https://developer.android.com/training/data-storage/room)

---

## ğŸš¨ Risk Mitigation

### Potential Issues & Solutions

**Issue 1: Whisper Integration Unavailable in RunAnywhere SDK**
- **Risk**: High
- **Impact**: Cannot transcribe speech
- **Mitigation**: 
  - Research SDK docs for STT support
  - Backup: Use Vosk (offline) or Android SpeechRecognizer
  - Time to pivot: 2 hours

**Issue 2: LLM Doesn't Generate Valid JSON**
- **Risk**: Medium
- **Impact**: Feedback parsing fails
- **Mitigation**:
  - Use few-shot examples in prompt
  - Add JSON validation and retry logic
  - Fallback to plain text parsing

**Issue 3: Model Too Large for Test Devices**
- **Risk**: Medium
- **Impact**: Out of memory errors
- **Mitigation**:
  - Use smaller models (Qwen 0.5B instead of 1.5B)
  - Test on multiple devices early
  - Document minimum requirements

**Issue 4: TTS Latency Too High**
- **Risk**: Low
- **Impact**: Breaks sub-80ms promise
- **Mitigation**:
  - Android TTS is fast (<200ms)
  - Clearly define latency metrics per component
  - "Sub-80ms" applies to VAD detection, not full pipeline

**Issue 5: Team Member Unavailable**
- **Risk**: Medium
- **Impact**: Delays in development
- **Mitigation**:
  - Clear task ownership and documentation
  - Cross-train on key components
  - Have backup plans for each role

**Issue 6: Demo Video Quality Poor**
- **Risk**: Low
- **Impact**: Bad first impression for judges
- **Mitigation**:
  - Record multiple takes
  - Use high-quality screen recording
  - Get feedback from team before finalizing

---

## ğŸ‰ Success Criteria

### Minimum Viable Product (MVP)

Must have for submission:
- [ ] User can speak into the app
- [ ] Speech is transcribed
- [ ] AI provides pronunciation feedback
- [ ] Feedback includes scores (pronunciation/grammar/fluency)
- [ ] User can see feedback on screen
- [ ] Basic UI is functional
- [ ] App runs without crashing
- [ ] Demo video shows end-to-end flow

### Target Product (Competitive)

To score well in hackathon:
- [ ] All MVP features âœ…
- [ ] Sub-80ms VAD detection latency
- [ ] Barge-in interruption works smoothly
- [ ] Professional UI with polish
- [ ] Session history and progress tracking
- [ ] Clear demonstration of on-device processing
- [ ] Excellent demo video
- [ ] Comprehensive README

### Stretch Goals (Wow Factor)

To win hackathon:
- [ ] All Target features âœ…
- [ ] Voice waveform visualization
- [ ] Multi-language support
- [ ] Pronunciation replay and comparison
- [ ] Gamification (streaks, achievements)
- [ ] Advanced analytics dashboard
- [ ] Performance benchmarks published
- [ ] Unit tests with high coverage

---

## ğŸ“ Getting Help

### Internal Resources
- This roadmap document
- `RUNANYWHERE_SDK_COMPLETE_GUIDE.md`
- `VAD_TESTING_GUIDE.md`
- `INTERRUPT_LOGIC_TESTING_GUIDE.md`

### External Resources
- **RunAnywhere Discord**: [Join link from hackathon page]
- **Hackathon Office Hours**: [Schedule from Unstop]
- **GitHub Issues**: For SDK bugs/questions

### Team Communication
- Daily standups: 15 minutes
- Async updates: [Your team chat platform]
- Code reviews: GitHub PR comments
- Blockers: Tag team immediately

---

## ğŸ Final Words

You've already completed **40% of the technical work** (VAD + Interrupt Logic)! 

The foundation is solid. Now it's time to:
1. **Connect the audio pipeline** (Member 1)
2. **Integrate the AI models** (Member 2)
3. **Build the UI and documentation** (Member 3)

**Key Success Factors**:
- Stay focused on the MVP first
- Test early and often on real devices
- Communicate constantly
- Don't get stuck - pivot if needed
- Have fun and learn!

**Remember**: Judges care about:
1. Does it work? (30%)
2. Why on-device? (25%)
3. Is it innovative? (25%)
4. Can we understand it? (10%)
5. Did you engage? (10%)

You're building something genuinely useful and technically impressive. 
The hard parts are done. Now execute on integration and polish.

**Let's win this hackathon! ğŸš€**

---

## ğŸ“š AI Agent Quick Reference

### Essential Commands

```bash
# Build project
./gradlew clean build

# Install on device
./gradlew installDebug

# Run app
adb shell am start -n com.pitchslap.app/.MainActivity

# View logs (filtered)
adb logcat -s PitchSlap_VAD:* PitchSlap_Interrupt:* PitchSlap_Audio:* PitchSlap_AI:* AndroidRuntime:E

# Clear logs
adb logcat -c

# Check app is running
adb shell ps | grep pitchslap

# Force stop app
adb shell am force-stop com.pitchslap.app

# Restart app
adb shell am force-stop com.pitchslap.app && adb shell am start -n com.pitchslap.app/.MainActivity
```

---

### Package Structure Reference

```
com.pitchslap.app/
â”œâ”€â”€ PitchSlapApplication.kt         # SDK init (âœ… Complete)
â”œâ”€â”€ MainActivity.kt                  # Test UI (âœ… Complete, will replace)
â”‚
â”œâ”€â”€ logic/                          # Core voice logic
â”‚   â”œâ”€â”€ VoiceActivityDetection.kt  # âœ… Complete (Phase 2)
â”‚   â””â”€â”€ InterruptLogic.kt          # âœ… Complete (Phase 3)
â”‚
â”œâ”€â”€ audio/                          # Audio processing
â”‚   â”œâ”€â”€ AudioRecorder.kt           # âŒ TODO (Phase 4.1)
â”‚   â””â”€â”€ TextToSpeechEngine.kt      # âŒ TODO (Phase 5.4)
â”‚
â”œâ”€â”€ ai/                             # AI integration
â”‚   â”œâ”€â”€ WhisperService.kt          # âŒ TODO (Phase 5.1)
â”‚   â””â”€â”€ FeedbackGenerator.kt       # âŒ TODO (Phase 5.3)
â”‚
â”œâ”€â”€ prompts/                        # System prompts
â”‚   â””â”€â”€ LanguageCoachPrompts.kt    # âŒ TODO (Phase 5.2)
â”‚
â”œâ”€â”€ data/                           # Data models
â”‚   â”œâ”€â”€ PronunciationFeedback.kt   # âŒ TODO (Phase 5.2)
â”‚   â””â”€â”€ ConversationTurn.kt        # âŒ TODO (Phase 6.1)
â”‚
â”œâ”€â”€ conversation/                   # Conversation orchestration
â”‚   â””â”€â”€ VoiceConversationManager.kt # âŒ TODO (Phase 6.1)
â”‚
â”œâ”€â”€ ui/                             # User interface
â”‚   â”œâ”€â”€ screens/                   # âŒ TODO (Phase 7.1)
â”‚   â”‚   â”œâ”€â”€ HomeScreen.kt
â”‚   â”‚   â”œâ”€â”€ PracticeScreen.kt
â”‚   â”‚   â”œâ”€â”€ FeedbackScreen.kt
â”‚   â”‚   â””â”€â”€ HistoryScreen.kt
â”‚   â”œâ”€â”€ navigation/                # âŒ TODO (Phase 7.1)
â”‚   â”‚   â””â”€â”€ NavGraph.kt
â”‚   â””â”€â”€ theme/                     # âœ… Complete
â”‚       â”œâ”€â”€ Color.kt
â”‚       â”œâ”€â”€ Theme.kt
â”‚       â””â”€â”€ Type.kt
â”‚
â””â”€â”€ data/                           # Database (optional)
    â”œâ”€â”€ SessionRepository.kt        # âŒ TODO (Phase 7.2)
    â”œâ”€â”€ SessionDao.kt               # âŒ TODO (Phase 7.2)
    â””â”€â”€ AppDatabase.kt              # âŒ TODO (Phase 7.2)
```

---

### Common Code Patterns

#### StateFlow Exposure Pattern

```kotlin
class MyComponent {
    private val _state = MutableStateFlow(initialValue)
    val state: StateFlow<Type> = _state.asStateFlow()
    
    fun updateState(newValue: Type) {
        _state.value = newValue
    }
}
```

#### Coroutine Launch Pattern

```kotlin
class MyComponent {
    private val scope = CoroutineScope(Dispatchers.IO + SupervisorJob())
    
    fun doAsyncWork() {
        scope.launch {
            try {
                // async work
            } catch (e: Exception) {
                Log.e(TAG, "Error: ${e.message}", e)
            }
        }
    }
    
    fun cleanup() {
        scope.cancel()
    }
}
```

#### UI State Collection Pattern

```kotlin
@Composable
fun MyScreen(viewModel: MyViewModel) {
    val state by viewModel.state.collectAsState()
    
    // Use state in UI
    Text(text = state.toString())
}
```

#### Dependency Injection Pattern

```kotlin
class MyComponent(
    private val dependency1: Dependency1,
    private val dependency2: Dependency2
) {
    // Use dependencies
}
```

---

### File Templates

#### New Kotlin Class Template

```kotlin
package com.pitchslap.app.[subpackage]

import android.util.Log
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow

/**
 * [Brief description of what this class does]
 * 
 * [Detailed explanation of purpose and usage]
 * 
 * @param [paramName] [param description]
 */
class ClassName(
    private val dependency: DependencyType
) {
    companion object {
        private const val TAG = "PitchSlap_ClassName"
    }
    
    // Public state
    private val _publicState = MutableStateFlow(initialValue)
    val publicState: StateFlow<Type> = _publicState.asStateFlow()
    
    /**
     * [Method description]
     * 
     * @param [paramName] [param description]
     * @return [return description]
     */
    fun publicMethod(param: Type): ReturnType {
        try {
            // Implementation
            Log.i(TAG, "âœ… Operation successful")
            return result
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Operation failed: ${e.message}", e)
            throw e
        }
    }
    
    /**
     * Cleanup resources
     */
    fun cleanup() {
        Log.i(TAG, "Cleaning up...")
        // Cleanup logic
    }
}
```

---

### Phase Checklist Summary

```
â˜ Phase 0: Cleanup (30min)
  â˜ Archive old summaries
  â˜ Delete Models.kt placeholder
  â˜ Create file inventory
  â˜ Create execution checklist
  â˜ Validate structure

â˜ Phase 4: Audio Recording (4-6h)
  â˜ Create AudioRecorder.kt
  â˜ Test audio recording in UI

â˜ Phase 5: AI Integration (8-12h)
  â˜ Integrate Whisper STT
  â˜ Design language coach prompts
  â˜ Create PronunciationFeedback data model
  â˜ Integrate LLM feedback generation
  â˜ Integrate TTS output

â˜ Phase 6: Conversation Manager (6-8h)
  â˜ Create VoiceConversationManager
  â˜ Update test UI for full pipeline

â˜ Phase 7: Production UI (8-10h)
  â˜ Design screen components
  â˜ Implement navigation
  â˜ Add progress tracking (Room)
  â˜ Polish and animations

â˜ Phase 8: Documentation (6-8h)
  â˜ Comprehensive testing
  â˜ Record demo video
  â˜ Write professional README
  â˜ Prepare submission package
```

---

### Critical Success Factors

1. **Follow the sequence**: Phase 0 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8
2. **Test incrementally**: After each file creation
3. **Log everything**: Use consistent TAG format
4. **Handle errors**: Try-catch on all external operations
5. **Document as you go**: KDoc comments mandatory
6. **Keep it clean**: No commented-out code, no TODOs in final version
7. **Verify integration**: Each component should connect smoothly
8. **Performance matters**: Profile if latency exceeds targets

---

### Emergency Recovery

If something breaks:

1. **Don't panic** - most issues are fixable
2. **Check Logcat** - error messages are your friend
3. **Isolate the issue** - which file/function is failing?
4. **Revert if needed** - Git is your safety net
5. **Rebuild from known good** - start from last working state
6. **Ask for help** - reference documentation or request clarification

---

### AI Agent Debugging Decision Tree

```
â”Œâ”€ Issue Encountered â”€â”
â”‚                      â”‚
â”‚  â”Œâ”€ Compilation Error?
â”‚  â”‚  â”œâ”€ YES â†’ Check imports, package declaration, Gradle sync
â”‚  â”‚  â””â”€ NO â†’ Continue
â”‚  â”‚
â”‚  â”Œâ”€ Runtime Crash?
â”‚  â”‚  â”œâ”€ YES â†’ Check Logcat, verify permissions, null safety
â”‚  â”‚  â””â”€ NO â†’ Continue
â”‚  â”‚
â”‚  â”Œâ”€ Feature Not Working?
â”‚  â”‚  â”œâ”€ YES â†’ Add more Log statements, verify state flow
â”‚  â”‚  â””â”€ NO â†’ Continue
â”‚  â”‚
â”‚  â”Œâ”€ Integration Issue?
â”‚  â”‚  â”œâ”€ YES â†’ Check dependency injection, StateFlow collection
â”‚  â”‚  â””â”€ NO â†’ Continue
â”‚  â”‚
â”‚  â”Œâ”€ Performance Problem?
â”‚  â”‚  â”œâ”€ YES â†’ Profile with Android Studio, check Dispatcher usage
â”‚  â”‚  â””â”€ NO â†’ Success! âœ…
â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Common Errors & Instant Fixes**:

| Error                             | Likely Cause                 | Fix                                           |
|-----------------------------------|------------------------------|-----------------------------------------------|
| `Unresolved reference`            | Missing import or dependency | Add import, check build.gradle.kts            |
| `Cannot access private member`    | Wrong visibility modifier    | Make class/function public                    |
| `StateFlow not updating UI`       | Not collecting in Composable | Use `.collectAsState()`                       |
| `Job is already complete`         | Coroutine scope cancelled    | Check lifecycle, use SupervisorJob            |
| `SecurityException: RECORD_AUDIO` | Permission not granted       | Check AndroidManifest.xml, runtime permission |
| `OutOfMemoryError`                | Model too large              | Use smaller model, check largeHeap            |

---

### File Dependency Graph (Read Before Creating Files)

```
Phase 4: AudioRecorder.kt
    â†“ depends on
    VoiceActivityDetection.kt (âœ… exists)

Phase 5: WhisperService.kt
    â†“ depends on
    AudioRecorder.kt (Phase 4)
    RunAnywhere SDK (âœ… exists)

Phase 5: PronunciationFeedback.kt
    â†“ depends on
    Nothing (standalone data class)

Phase 5: LanguageCoachPrompts.kt
    â†“ depends on
    PronunciationFeedback.kt (same phase)

Phase 5: FeedbackGenerator.kt
    â†“ depends on
    PronunciationFeedback.kt (same phase)
    LanguageCoachPrompts.kt (same phase)
    RunAnywhere SDK (âœ… exists)

Phase 5: TextToSpeechEngine.kt
    â†“ depends on
    InterruptLogic.kt (âœ… exists)

Phase 6: ConversationTurn.kt
    â†“ depends on
    PronunciationFeedback.kt (Phase 5)

Phase 6: VoiceConversationManager.kt
    â†“ depends on
    VoiceActivityDetection.kt (âœ… exists)
    InterruptLogic.kt (âœ… exists)
    AudioRecorder.kt (Phase 4)
    WhisperService.kt (Phase 5)
    FeedbackGenerator.kt (Phase 5)
    TextToSpeechEngine.kt (Phase 5)

Phase 7: All UI Screens
    â†“ depends on
    VoiceConversationManager.kt (Phase 6)
```

**Rule**: Never create a file if its dependencies aren't complete!

---

### Final Pre-Flight Check

Before considering the project complete:

- [ ] All phases marked complete
- [ ] `./gradlew build` succeeds with zero warnings
- [ ] App runs on physical device without crashes
- [ ] End-to-end voice flow works (speak â†’ feedback â†’ repeat)
- [ ] Barge-in interruption works flawlessly
- [ ] All Logcat logs are meaningful and error-free
- [ ] README.md is professional and complete
- [ ] Demo video showcases all features
- [ ] Submission package ready for Unstop

---

**ğŸ¯ Remember**: Quality over quantity. A working MVP beats a broken full-feature app.

---

*Last Updated: [Current Date]*  
*Version: 2.0 - AI Agent Optimized*  
*Next Review: After each phase completion*
